"""
RAG Bot Query Interface

This script provides the query interface for the RAG (Retrieval-Augmented Generation) system.
It allows users to ask questions about their PDF documents and receive contextual answers
generated by combining vector similarity search with large language model capabilities.

Features:
- Semantic search across document embeddings using ChromaDB
- Context-aware answer generation using Ollama's Mistral model
- Source attribution showing which documents contributed to the answer
- Command-line interface for easy integration

Usage:
    python query_data.py "What is the main topic discussed in the documents?"
    python query_data.py "Explain the methodology used in the research"
"""

import argparse
from langchain.vectorstores.chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from get_embedding_function import get_embedding_function

# Configuration constants
CHROMA_PATH = "chroma"  # Directory where ChromaDB vector store is persisted

# Prompt template for the language model
# This template structures how context and questions are presented to the LLM
PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

___

Answer the question based on the above context: {question}
"""


def main():
    """
    Main entry point for the query interface.

    Parses command line arguments to get the user's question and
    passes it to the RAG query function for processing.
    """
    # Set up command line argument parsing
    parser = argparse.ArgumentParser()
    parser.add_argument("query_text", type=str, help="The query text.")
    args = parser.parse_args()
    query_text = args.query_text

    # Process the query through the RAG system
    query_rag(query_text)


def query_rag(query_text: str):
    """
    Execute a RAG (Retrieval-Augmented Generation) query against the document database.

    This is the core function that implements the RAG pipeline:
    1. Converts the user's question into a vector embedding
    2. Searches the ChromaDB vector store for similar document chunks
    3. Combines the most relevant chunks as context
    4. Generates an answer using the LLM with the retrieved context
    5. Returns the answer along with source attribution

    Args:
        query_text (str): The user's question or query
    """
    # Initialize embedding function and connect to vector database
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # Perform similarity search to find relevant document chunks
    # k=5 means we retrieve the top 5 most similar chunks
    results = db.similarity_search_with_score(query_text, k=5)

    # Combine the content of retrieved chunks into a single context string
    # Each chunk is separated by a clear delimiter for better LLM understanding
    context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])

    # Prepare the prompt for the language model
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, question=query_text)

    # Generate response using Ollama's Mistral model
    model = Ollama(model="mistral")
    response_text = model.invoke(prompt)

    # Extract source information for attribution
    # This allows users to verify and explore the original documents
    chunk_ids = [doc.metadata.get("id", None) for doc, _score in results]

    # Format and display the response with clear visual separators
    formatted_response = (
        f"\n{'='*100}\n"
        f"Response: {response_text}\n"
        f"{'='*100}\n"
        f"Sources: {chunk_ids}"
    )

    print(formatted_response)


if __name__ == "__main__":
    main()
